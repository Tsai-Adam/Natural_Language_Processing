{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_true_labels = pd.read_csv('data/test_true_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is hard to find such delightful and adorabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>They don't make films like this faded, hauntin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I doubt that I'd ever seen anything resembling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I was shocked to find myself riveted to this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The makers of this movie really touched a sore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I Care A Lot is an exhilarating black comedy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Really loved this. This film is masterful in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The story, direction and acting across the boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10754</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This movie ruled! It had such a unique premise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10755 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "0                  1.0  It is hard to find such delightful and adorabl...\n",
       "1                  1.0  They don't make films like this faded, hauntin...\n",
       "2                  1.0  I first viewed this movie in 1924 at age 6 yrs...\n",
       "3                  1.0  I doubt that I'd ever seen anything resembling...\n",
       "4                  1.0  I was shocked to find myself riveted to this m...\n",
       "...                ...                                                ...\n",
       "10750              0.0  The makers of this movie really touched a sore...\n",
       "10751              0.0  I Care A Lot is an exhilarating black comedy w...\n",
       "10752              0.0  Really loved this. This film is masterful in t...\n",
       "10753              0.0  The story, direction and acting across the boa...\n",
       "10754              0.0  This movie ruled! It had such a unique premise...\n",
       "\n",
       "[10755 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_tagged = [\n",
    "    TaggedDocument(words=wordpunct_tokenize(text.lower()), tags=[str(i)])\n",
    "    for i, text in enumerate(train_df[\"imdb_user_review\"])\n",
    "]\n",
    "\n",
    "# testing data（no tags）\n",
    "test_tagged = [\n",
    "    wordpunct_tokenize(text.lower())\n",
    "    for text in test_df[\"imdb_user_review\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the doc2vec model\n",
    "model_d2v = Doc2Vec(vector_size=128, window=3, min_count=1, workers=4, epochs=100)\n",
    "model_d2v.build_vocab(train_tagged)\n",
    "model_d2v.train(train_tagged, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# encode\n",
    "X_train = np.array([model_d2v.dv[str(i)] for i in range(len(train_tagged))])\n",
    "y_train = train_df[\"helpfulness_cat\"].values\n",
    "\n",
    "X_test = np.array([model_d2v.infer_vector(words) for words in test_tagged])\n",
    "y_test = test_true_labels[\"helpfulness_cat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/adam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define preprocessing function:\n",
    "# - lowercase and tokenize\n",
    "# - remove stopwords\n",
    "# - apply stemming\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each review\n",
    "X_train_tokens = train_df[\"imdb_user_review\"].astype(str).apply(preprocess)\n",
    "X_test_tokens = test_df[\"imdb_user_review\"].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dictionary and corpus (based only on the training set)\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Create a dictionary and bag-of-words corpus for the training set\n",
    "dictionary = Dictionary(X_train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in X_train_tokens]\n",
    "test_corpus = [dictionary.doc2bow(tokens) for tokens in X_test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training lda_models 25\n"
     ]
    }
   ],
   "source": [
    "### Train multiple LDA models with different numbers of topics\n",
    "\n",
    "#topic_nums = [25]\n",
    "topic_nums = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "lda_models = {}\n",
    "\n",
    "for num_topics in topic_nums:\n",
    "    lda = LdaModel(corpus=train_corpus,\n",
    "                   id2word=dictionary,\n",
    "                   num_topics=num_topics,\n",
    "                   passes=10,\n",
    "                   random_state=38)\n",
    "    lda_models[num_topics] = lda\n",
    "    print(f\"Finish training lda_models {num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert each review to a topic distribution vector\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_topic_vector(model, corpus, num_topics):\n",
    "    topic_vecs = []\n",
    "    for doc_bow in corpus:\n",
    "        doc_topics = model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        vec = [prob for _, prob in sorted(doc_topics)]\n",
    "        topic_vecs.append(vec)\n",
    "    return np.array(topic_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_topics  accuracy  f1_score       auc\n",
      "4          25  0.773615  0.871732  0.672734\n",
      "1          10  0.772234  0.871424  0.651568\n",
      "0           5  0.772037  0.871355  0.643560\n",
      "3          20  0.772037  0.871355  0.658504\n",
      "6          35  0.772037  0.871183  0.656774\n",
      "2          15  0.772629  0.871101  0.656515\n",
      "7          40  0.771248  0.870420  0.652376\n",
      "5          30  0.768290  0.868847  0.652619\n"
     ]
    }
   ],
   "source": [
    "### Train classifiers and evaluate performance for each topic model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "results = []\n",
    "\n",
    "for num_topics, lda_model in lda_models.items():\n",
    "    X_train_vec = get_topic_vector(lda_model, train_corpus, num_topics)\n",
    "    X_test_vec = get_topic_vector(lda_model, test_corpus, num_topics)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    y_prob = clf.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "    results.append({\n",
    "        \"num_topics\": num_topics,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_prob)\n",
    "    })\n",
    "\n",
    "# Show results sorted by F1-score\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by=\"f1_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_topic_vector(lda_models[25], train_corpus, num_topics)\n",
    "X_test = get_topic_vector(lda_models[25], test_corpus, num_topics)\n",
    "y_train = train_df[\"helpfulness_cat\"]\n",
    "y_test = test_true_labels[\"helpfulness_cat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c0e2eb2884448690d77cc9342092b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7457daa70c473096f4680704618d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "# encode\n",
    "X_train = model.encode(train_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)\n",
    "X_test = model.encode(test_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)\n",
    "y_train = train_df[\"helpfulness_cat\"]\n",
    "y_test = test_true_labels[\"helpfulness_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10755, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using 'all-MiniLM-L6-v2' and 'paraphrase-mpnet-base-v2' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477a9d9818ce4e4c8ddbf975cd03ca08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4364754a0d2d4e0cb446a55bfca59fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0aa351663645f689608f65f43a384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459b668659b747f1b36b92404882e1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# set model\n",
    "model1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model2 = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "# encode\n",
    "emb1_train = model1.encode(train_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)\n",
    "emb1_test = model1.encode(test_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)\n",
    "emb2_train = model2.encode(train_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)\n",
    "emb2_test = model2.encode(test_df[\"imdb_user_review\"].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10755, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic modeling probability with 25 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/adam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize stopword set and stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocessing function:\n",
    "# - lowercase and tokenize - remove stopwords - apply stemming\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to training and testing reviews\n",
    "X_train_tokens = train_df[\"imdb_user_review\"].astype(str).apply(preprocess)\n",
    "X_test_tokens = test_df[\"imdb_user_review\"].astype(str).apply(preprocess)\n",
    "\n",
    "# Create dictionary and bag-of-words corpus\n",
    "dictionary = Dictionary(X_train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in X_train_tokens]\n",
    "test_corpus = [dictionary.doc2bow(tokens) for tokens in X_test_tokens]\n",
    "\n",
    "# Train LDA model with 25 topics\n",
    "num_topics = 25\n",
    "lda = LdaModel(corpus=train_corpus,\n",
    "               id2word=dictionary,\n",
    "               num_topics=num_topics,\n",
    "               passes=10,\n",
    "               random_state=38)\n",
    "\n",
    "# Function to convert each document into a topic distribution vector\n",
    "def get_topic_vector(model, corpus, num_topics):\n",
    "    topic_vecs = []\n",
    "    for doc_bow in corpus:\n",
    "        doc_topics = model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        vec = [prob for _, prob in sorted(doc_topics)]\n",
    "        topic_vecs.append(vec)\n",
    "    return np.array(topic_vecs)\n",
    "\n",
    "# Get topic distribution vectors for training and testing sets\n",
    "X_train_topic = get_topic_vector(lda, train_corpus, num_topics)\n",
    "X_test_topic = get_topic_vector(lda, test_corpus, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10755, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[\"imdb_user_review\"])\n",
    "X_test_tfidf = tfidf.transform(test_df[\"imdb_user_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10755, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conbine vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conbined vector\n",
    "X_train = np.concatenate([emb1_train, emb2_train, X_train_topic, X_train_tfidf.toarray()], axis=1)\n",
    "X_test = np.concatenate([emb1_test, emb2_test, X_test_topic, X_test_tfidf.toarray()], axis=1)\n",
    "y_train = train_df[\"helpfulness_cat\"]\n",
    "y_test = test_true_labels[\"helpfulness_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10755, 1277)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 0.7809110629067245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"RF Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7677 - loss: 0.5443 - val_accuracy: 0.7007 - val_loss: 0.5896\n",
      "Epoch 2/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - accuracy: 0.7804 - loss: 0.4654 - val_accuracy: 0.7072 - val_loss: 0.5975\n",
      "Epoch 3/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.7964 - loss: 0.4478 - val_accuracy: 0.7072 - val_loss: 0.6182\n",
      "Epoch 4/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - accuracy: 0.8143 - loss: 0.4145 - val_accuracy: 0.7063 - val_loss: 0.6197\n",
      "Epoch 5/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 0.8335 - loss: 0.3788 - val_accuracy: 0.7035 - val_loss: 0.6241\n",
      "Epoch 6/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step - accuracy: 0.8518 - loss: 0.3492 - val_accuracy: 0.6803 - val_loss: 0.6573\n",
      "Epoch 7/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 0.8747 - loss: 0.3097 - val_accuracy: 0.7045 - val_loss: 0.6754\n",
      "Epoch 8/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 0.8901 - loss: 0.2760 - val_accuracy: 0.6924 - val_loss: 0.7234\n",
      "Epoch 9/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - accuracy: 0.8984 - loss: 0.2569 - val_accuracy: 0.7007 - val_loss: 0.7637\n",
      "Epoch 10/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 0.9219 - loss: 0.2111 - val_accuracy: 0.6914 - val_loss: 0.8191\n",
      "Epoch 11/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.9341 - loss: 0.1867 - val_accuracy: 0.6887 - val_loss: 0.9041\n",
      "Epoch 12/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - accuracy: 0.9465 - loss: 0.1569 - val_accuracy: 0.6784 - val_loss: 0.9553\n",
      "Epoch 13/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9529 - loss: 0.1336 - val_accuracy: 0.6924 - val_loss: 1.0558\n",
      "Epoch 14/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9649 - loss: 0.1066 - val_accuracy: 0.6877 - val_loss: 1.1145\n",
      "Epoch 15/15\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - accuracy: 0.9788 - loss: 0.0806 - val_accuracy: 0.6812 - val_loss: 1.2146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x43c90ab40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model_nn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_nn.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "input_shape=(X_train.shape[1],)\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = Dense(384, activation=None, kernel_regularizer=l2(0.01))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Dropout(0.5)(x)  # 50% dropout\n",
    "\n",
    "x = Dense(192, activation=None, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Dropout(0.5)(x)  # 50% dropout\n",
    "\n",
    "x = Dense(96, activation=None, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Dropout(0.5)(x)  # 50% dropout\n",
    "\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7131 - loss: 6.7923 - val_accuracy: 0.7268 - val_loss: 2.0354\n",
      "Epoch 2/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7786 - loss: 1.5969 - val_accuracy: 0.7454 - val_loss: 1.0745\n",
      "Epoch 3/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.9316 - val_accuracy: 0.7212 - val_loss: 0.8954\n",
      "Epoch 4/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7857 - loss: 0.7771 - val_accuracy: 0.7435 - val_loss: 0.8720\n",
      "Epoch 5/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.7706 - val_accuracy: 0.7305 - val_loss: 0.8638\n",
      "Epoch 6/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7866 - loss: 0.7253 - val_accuracy: 0.6803 - val_loss: 0.8793\n",
      "Epoch 7/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7926 - loss: 0.7282 - val_accuracy: 0.7100 - val_loss: 0.8483\n",
      "Epoch 8/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7858 - loss: 0.7188 - val_accuracy: 0.7045 - val_loss: 0.8344\n",
      "Epoch 9/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7838 - loss: 0.7080 - val_accuracy: 0.7156 - val_loss: 0.8362\n",
      "Epoch 10/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7861 - loss: 0.7287 - val_accuracy: 0.6933 - val_loss: 0.8796\n",
      "Epoch 11/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.7282 - val_accuracy: 0.7454 - val_loss: 0.8180\n",
      "Epoch 12/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7913 - loss: 0.7176 - val_accuracy: 0.7416 - val_loss: 0.8706\n",
      "Epoch 13/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7893 - loss: 0.7307 - val_accuracy: 0.7045 - val_loss: 0.8912\n",
      "Epoch 14/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.7317 - val_accuracy: 0.7193 - val_loss: 0.8732\n",
      "Epoch 15/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7768 - loss: 0.7251 - val_accuracy: 0.7323 - val_loss: 0.8409\n",
      "Epoch 16/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7914 - loss: 0.7039 - val_accuracy: 0.6877 - val_loss: 0.9047\n",
      "Epoch 17/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7834 - loss: 0.7576 - val_accuracy: 0.6691 - val_loss: 0.9017\n",
      "Epoch 18/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7874 - loss: 0.7258 - val_accuracy: 0.7230 - val_loss: 0.8162\n",
      "Epoch 19/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.7209 - val_accuracy: 0.7305 - val_loss: 0.8409\n",
      "Epoch 20/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7976 - loss: 0.6986 - val_accuracy: 0.7193 - val_loss: 0.8705\n",
      "Epoch 21/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7861 - loss: 0.7099 - val_accuracy: 0.7398 - val_loss: 0.7925\n",
      "Epoch 22/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7938 - loss: 0.7007 - val_accuracy: 0.6989 - val_loss: 0.8558\n",
      "Epoch 23/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7852 - loss: 0.7130 - val_accuracy: 0.7175 - val_loss: 0.8112\n",
      "Epoch 24/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7916 - loss: 0.7070 - val_accuracy: 0.7100 - val_loss: 0.8262\n",
      "Epoch 25/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7916 - loss: 0.7051 - val_accuracy: 0.7268 - val_loss: 0.8232\n",
      "Epoch 26/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7860 - loss: 0.6875 - val_accuracy: 0.6970 - val_loss: 0.8034\n",
      "Epoch 27/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7905 - loss: 0.6869 - val_accuracy: 0.7082 - val_loss: 0.8602\n",
      "Epoch 28/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7874 - loss: 0.7007 - val_accuracy: 0.6822 - val_loss: 0.8295\n",
      "Epoch 29/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7874 - loss: 0.6901 - val_accuracy: 0.6543 - val_loss: 0.8813\n",
      "Epoch 30/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7883 - loss: 0.6908 - val_accuracy: 0.7119 - val_loss: 0.7991\n",
      "Epoch 31/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7918 - loss: 0.6753 - val_accuracy: 0.7286 - val_loss: 0.8058\n",
      "Epoch 32/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7956 - loss: 0.6726 - val_accuracy: 0.7082 - val_loss: 0.8023\n",
      "Epoch 33/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7963 - loss: 0.6769 - val_accuracy: 0.7379 - val_loss: 0.8511\n",
      "Epoch 34/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7947 - loss: 0.6690 - val_accuracy: 0.7286 - val_loss: 0.7631\n",
      "Epoch 35/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7940 - loss: 0.6683 - val_accuracy: 0.7454 - val_loss: 0.7446\n",
      "Epoch 36/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7945 - loss: 0.6447 - val_accuracy: 0.7007 - val_loss: 0.7906\n",
      "Epoch 37/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8037 - loss: 0.6360 - val_accuracy: 0.7212 - val_loss: 0.7721\n",
      "Epoch 38/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7858 - loss: 0.6467 - val_accuracy: 0.7045 - val_loss: 0.7763\n",
      "Epoch 39/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.6551 - val_accuracy: 0.7472 - val_loss: 0.7913\n",
      "Epoch 40/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7947 - loss: 0.6431 - val_accuracy: 0.7286 - val_loss: 0.8038\n",
      "Epoch 41/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7937 - loss: 0.6569 - val_accuracy: 0.7193 - val_loss: 0.7715\n",
      "Epoch 42/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7992 - loss: 0.6301 - val_accuracy: 0.6933 - val_loss: 0.7886\n",
      "Epoch 43/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7984 - loss: 0.6246 - val_accuracy: 0.6933 - val_loss: 0.7868\n",
      "Epoch 44/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7901 - loss: 0.6328 - val_accuracy: 0.7230 - val_loss: 0.7499\n",
      "Epoch 45/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7935 - loss: 0.6174 - val_accuracy: 0.6859 - val_loss: 0.7456\n",
      "Epoch 46/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7847 - loss: 0.6256 - val_accuracy: 0.7342 - val_loss: 0.8120\n",
      "Epoch 47/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7870 - loss: 0.6138 - val_accuracy: 0.6766 - val_loss: 0.7631\n",
      "Epoch 48/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7948 - loss: 0.6149 - val_accuracy: 0.7323 - val_loss: 0.7415\n",
      "Epoch 49/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7913 - loss: 0.6171 - val_accuracy: 0.7268 - val_loss: 0.7395\n",
      "Epoch 50/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7943 - loss: 0.6135 - val_accuracy: 0.7286 - val_loss: 0.7127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x6e9e57ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step\n"
     ]
    }
   ],
   "source": [
    "# predict the testing data\n",
    "y_pred_nn = model.predict(X_test)\n",
    "\n",
    "# convert probability to 0 and 1, using threshold = 0.5\n",
    "y_pred_nn_binary = (y_pred_nn > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Accuracy: 0.7990534411358706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"NN Accuracy:\", accuracy_score(y_test, y_pred_nn_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.40      0.47      1156\n",
      "         1.0       0.84      0.92      0.88      3915\n",
      "\n",
      "    accuracy                           0.80      5071\n",
      "   macro avg       0.71      0.66      0.68      5071\n",
      "weighted avg       0.78      0.80      0.78      5071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print classification report (includes precision, recall, f1-score, and support)\n",
    "print(classification_report(y_test, y_pred_nn_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
